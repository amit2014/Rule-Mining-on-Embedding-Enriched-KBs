{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Learning on KB Enriched with Embedding-Based and Text-Based Similarity Function\n",
    "**By:** Vinay Chitepu <br/>\n",
    "**Advisors**: Daisy Wang, Ali Sadeghian  <br/>\n",
    "**Course:** CAP4773-Projects in Data Science <br/>\n",
    "**UF Data Science Research Lab**\n",
    "\n",
    "## Introduction\n",
    "Background, project proposal, and summaries of papers\n",
    "\n",
    "\n",
    "### Background\n",
    "\n",
    "Knowledge Bases (KBs) are structures that hold a large variety of information about different entities and facts that determine relationships between these entities. While they are large they are not complete i.e. they don’t have all the relationships that are possible (they don’t have all the rules/facts that can be determined). This will focus on RDF knowledge bases which are KBs such as Freebase, YAGO, and DBPedia.\n",
    "\n",
    "Knowledge Embedding has been used as a way to extract features using a low-dimensional continuous space. It can be used to derive unknown faces from KBs and see if the existing triples in the KBs are also correct. \n",
    "\n",
    "Rule Mining is the process of analyzing mined data for patterns and occurrences and finds frequent associations between different entities and previous rules to establish new rules.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "Explore enriching KBs with additional links obtained from embedding based methods.\n",
    "- Look at different embedding methods to determine which to be the best to use in for this purpose.\n",
    "- Research rule mining systems (AIME paper) to determine which system(s) to use for this project\n",
    "\n",
    "Analyze the effect of these links on quality and expressiveness of rules.\n",
    "- Test embeddings and rule minings on smaller dataset to get some practice.\n",
    "- Test embeddings on FB15K-237 and YAGO.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Seeing if rule learning on a KB enriched with embedding based links, such as similar_to or related_to, help with the accuracy of the rules. Today, there exist rule mining systems and software but adding these similarity based links may increase the accuracy of these links.\n",
    "\n",
    "\n",
    "### Related Work\n",
    "\n",
    "There are many embedding methods currently in use, tensor based as well as many different rule mining systems in use. Some embedding methods are Structure Embedding, Neural Tensor Networks, Translation-Based, and Bilinear-Diag Model. Some Rule Mining methods include Associative Rule Mining and Logical Rule Mining. \n",
    "\n",
    "\n",
    "### Proposed Methods\n",
    "\n",
    "Use an embedding method such as DistMult to create similarity-based links in the knowledge base and then use a rule mining to determine an ontology for the KB. Test to see if the accuracy of the rules is better than just using only the rule mining method in place today. Using the OpenKE package for the Knowledge Embedding that has implementations of the embedding methods. <br><br>\n",
    "\n",
    "Another method is to enrich the knowledge base useing text-based embeddings from the English-Wiki using Word2Vec embeddings and cosine similarity\n",
    "\n",
    "\n",
    "### Evaluation Datasets\n",
    "- Freebase (FB15K)\n",
    "- YAGO2s\n",
    "\n",
    "### Benchmark\n",
    "Current rule mining software works pretty well but adding the embedding based links might make their accuracy better.\n",
    "\n",
    "## OpenKE for Embedding\n",
    "I will use the OpenKE (Open-Source Package for Knowledge Embedding) to implement the embedding models in training and testing\n",
    "\n",
    "#### Installation for OpenKE\n",
    "\n",
    "```\n",
    "$ git clone https://github.com/thunlp/OpenKE \n",
    "$ cd OpenKE \n",
    "$ bash make.sh\n",
    "```\n",
    "\n",
    "#### Installation for Tensorflow\n",
    "\n",
    "```\n",
    "$ pip3 install --upgrade tensorflow\n",
    "```\n",
    "\n",
    "### Formatting Data\n",
    "Formatted files are location in directory as this IPython Notebook for reerence\n",
    "\n",
    "#### Training \n",
    "3 Files\n",
    "- **entity2id.txt:** Entity file; the first line is the number of entities. The following lines contain the entities id followed by unique integer to represent it. \n",
    "- **relation2id.txt:** Relation file; the first line represents the number of relationships. The following lines contain the full relationships followed by a unique interger to represent it.\n",
    "\n",
    "-  **train2id.txt:** Training file; the first line is the number of triples for training. Then the following lines are all in the format (e1, e2, rel) which indicates there is a relation rel between e1 and e2. NOTE: The training file uses the unique integer ids that are set in the entity2id.txt and relation2id.txt files rather than the actual id and relation.\n",
    "    - e1 = subject\n",
    "    - e2 = object\n",
    "    - rel = relation\n",
    "    \n",
    "#### Testing\n",
    "2 Files\n",
    "- **test2id.txt:** Testing file. The first line is the number of triples for testing. The following lines are all in the format (e1, e2, rel).\n",
    "- **valid2id.txt:** Validating file. The first line is the number of triples for validating. The following lines are all in the format (e1, e2, rel).\n",
    "\n",
    "#### Other\n",
    "- **type_constrain.txt:** Type contrain file. The first line is the number of relations. The following are the constraint type for each relation.\n",
    "\n",
    "\n",
    "## Word2Vec Models\n",
    "\n",
    "```\n",
    "$ pip3 install gensim\n",
    "```\n",
    "\n",
    "## Importing Modules, Libraries and Data\n",
    "Importing Python and OpenKE models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From OpenKE\n",
    "from OpenKE import models, config\n",
    "\n",
    "# From Python3\n",
    "import os                         # Running terminal commands\n",
    "import random                     # Random Number Generator\n",
    "import numpy as np                # Math library\n",
    "import pandas as pd               # Data Analytics and DataFrames\n",
    "import seaborn as sns             # Visualization\n",
    "import multiprocessing            # Parallel Processing\n",
    "import tensorflow as tf           # Tensorflow ML library\n",
    "import gensim.models as w2v       # Word2Vec Library\n",
    "import matplotlib.pyplot as plt   # Plotting\n",
    "from sklearn import preprocessing # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('FB15K/train.txt', sep = '\\t', names = ['Entity', 'Relation', 'Tail'])\n",
    "test_df = pd.read_csv('FB15K/test.txt', sep = '\\t', names = ['Entity', 'Relation', 'Tail'])\n",
    "valid_df = pd.read_csv('FB15K/valid.txt', sep = '\\t', names = ['Entity', 'Relation', 'Tail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59071"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483142"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework\n",
    "Evaluation framework using Hits@10. Pass output file from java eval file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_frame(file, test_len):\n",
    "    \n",
    "    # Open file\n",
    "    f = open(file)\n",
    "    \n",
    "    # Hits counter\n",
    "    hits = 0\n",
    "    \n",
    "    # Loop though all facts in KB\n",
    "    for x in range(test_len):\n",
    "\n",
    "        # Read line\n",
    "        fact = f.readline()\n",
    "        fact = fact.split(' ')\n",
    "        if fact != ['']:\n",
    "            # Get target head and tail\n",
    "            head_target = fact[0]\n",
    "            tail_target = fact[2][:-1]\n",
    "\n",
    "\n",
    "            # Get head predictions\n",
    "            headpreds = f.readline()\n",
    "            headpreds = headpreds.split(' ')\n",
    "            headpreds = headpreds[1].split('\\t')\n",
    "            headpreds.pop()\n",
    "\n",
    "            # Get tail predictions\n",
    "            tailpreds = f.readline()\n",
    "            tailpreds = tailpreds.split(' ')\n",
    "            tailpreds = tailpreds[1].split('\\t')\n",
    "            tailpreds.pop()\n",
    "\n",
    "\n",
    "            if (head_target in headpreds) and (tail_target in tailpreds):\n",
    "                if (len(headpreds) < 10) and (len(tailpreds) < 10):\n",
    "                    hits+=1\n",
    "        else:\n",
    "            print('miss')\n",
    "                \n",
    "    return hits/(test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline for FB15K\n",
    "Using the AMIE Rule-Mining system to mine rules on the baseline FB15K dataset. Then evaluating the rules using Hits@10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule Mining using AMIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('java -XX:-UseGCOverheadLimit -Xmx4g -jar AMIE/amie_plus.jar -minhc 0.0 -mins 0 -minis 0 FB15K/train.txt > rules/baseline_rules.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Remember to clean 'baseline_rules.txt' so it only includes the rule. Take out output from top and bottom so there are only rules.\n",
    "\n",
    "#### Evaluating Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('java -jar AMIE/ApplyAMIERules.jar rules/baseline_rules.txt FB15K/train.txt FB15K/test.txt FB15K/valid.txt evaluation/baseline_rules_eval.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@10: 0.8082815594792707\n"
     ]
    }
   ],
   "source": [
    "print('Hits@10: ' + str(eval_frame('evaluation/baseline_rules_eval.txt', len(test_df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistMult Graph Embeddings\n",
    "Embedding the entities in FB15K using DistMult. Below is the neural network architecture for DistMult Embedding.\n",
    "\n",
    "<img src=\"images/nnarch.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding using DistMult in OpenKE Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = config.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set_in_path('./benchmarks/FB15K/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set_test_link_prediction(True)\n",
    "con.set_test_triple_classification(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocate CPU Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set_work_threads(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Parameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set_train_times(500)  # To set the data traversing rounds\n",
    "con.set_nbatches(100)     # To split the training triples into several batches\n",
    "con.set_alpha(0.1)        # To set the learning rate\n",
    "con.set_dimension(100)    # To set the dimensions of the entities and relations at the same time\n",
    "# con.set_margin(1)         # To set the margin for the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set_bern(0)            # To set negative sampling algorithms, unif (bern = 0) or bern (bern = 1)\n",
    "con.set_ent_neg_rate(1)   # For each positive triple, we construct rate negative triples by corrupt the entity\n",
    "con.set_rel_neg_rate(0)    # For each positive triple, we construct rate negative triples by corrupt the relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Optimization Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set_opt_method(\"Adagrad\")  # To set the gradient descent optimization algorithm (SGD, Adagrad, Adadelta, Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set_export_files(\"./res/model.vec.tf\", 0)  # To set the export file of model paramters, every few rounds\n",
    "con.set_out_files(\"./res/embedding.vec.json\")  # To export model parameters to json files when training completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.init()                     # Initialize the experimental settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Links based on Graph Embeddings (Cosine Similarity)\n",
    "Using the DistMult graph embedding and cosine similarity in order to determine and generate similarity links and introduce them into the graph. The method used in shown below:\n",
    "<img src=\"images/cossim.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine-Similarity Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product\n",
    "def dot(x,y):\n",
    "    return np.sum(x * y) \n",
    "\n",
    "# Vector Magnitude\n",
    "def mag(x):\n",
    "    return np.sqrt(np.sum(x * x))\n",
    "\n",
    "# Cosine Similarity\n",
    "def cosine_similar_to(h,t,ent_embeddings):\n",
    "    ent_h = ent_embeddings[h]\n",
    "    ent_t = ent_embeddings[t]\n",
    "    cos_sim = np.absolute(dot(ent_h,ent_t)) / (mag(ent_h) * mag(ent_t))\n",
    "    return(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_links_07 = pd.read_csv('./cos_sim_links/cos_sim_links_07.tsv', sep = '\\t', index_col=0)\n",
    "sim_links_085 = pd.read_csv('./cos_sim_links/cos_sim_links_085.tsv', sep = '\\t', index_col=0)\n",
    "sim_links_09 = pd.read_csv('./cos_sim_links/cos_sim_links_09.tsv', sep = '\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_links_07_enriched = train_df.append(sim_links_07)\n",
    "sim_links_085_enriched = train_df.append(sim_links_085)\n",
    "sim_links_09_enriched = train_df.append(sim_links_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_links_07_enriched.to_csv('./graph_enriched_data/sim_links_07_enriched.txt', sep='\\t', index=False)\n",
    "sim_links_085_enriched.to_csv('./graph_enriched_data/sim_links_085_enriched.txt', sep='\\t', index=False)\n",
    "sim_links_09_enriched.to_csv('./graph_enriched_data/sim_links_09_enriched.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('java -XX:-UseGCOverheadLimit -Xmx4g -jar AMIE/amie_plus.jar -minhc 0.0 -mins 0 -minis 0 ./graph_enriched_data/sim_links_09_enriched.txt > rules/graph_rules.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('java -jar AMIE/ApplyAMIERules.jar rules/graph_rules.txt ./graph_enriched_data/sim_links_09_enriched.txt FB15K/test.txt FB15K/valid.txt evaluation/graph_rules_eval.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@10: 0.8027119906553131\n"
     ]
    }
   ],
   "source": [
    "print('Hits@10: ' + str(eval_frame('evaluation/graph_rules_eval.txt', len(test_df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Graph Embedding\n",
    "Using built-in method to obtain graph embedding. This will be used for cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = con.get_parameters('numpy')\n",
    "embeddings = embeddingsbeddingsbeddings['ent_embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating similar_to links \n",
    "\n",
    "Generates similar_to link based on cosine similiarity above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cos_sim_links(iterations = 100000, threshold = 0.8, embeddings = embeddings):\n",
    "    \n",
    "    h_list = []; t_list = []; acc_list = []\n",
    "    for _ in range(iterations):\n",
    "        h = random.randint(1, 14540)\n",
    "        t = random.randint(1, 14540)\n",
    "        acc = cosine_similar_to(h,t,embeddings)\n",
    "        if acc > threshold:\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)       \n",
    "    \n",
    "    d = {'head': h_list, 'tail': t_list}\n",
    "    return pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gen_cos_sim_links(iterations=100000, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Links based on Word2Vec Embeddings (Cosine Similarity)\n",
    "Using text-based information from the English Wikipedia to create a Word2Vec model. Using cosine similairty from before on these embeddings in order to determing similarity links creation. The gensim module used is nice and automatically will do cos-sim for us so we don't have to call additional functions. Below is the network architecture for the skip-gram embedding model.\n",
    "<img src=\"images/sgnn.png\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from Freebase RDF m.id to Real Name\n",
    "Right now all the entities are referenced as m.id from Freebase. We will convert these to thier real names using the freebase mapping file referenced in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mapping_df = pd.read_csv('FB15K/freebase-entities.tsv', '\\t', header=None, names=['fb','string'])\n",
    "\n",
    "keys = list(fb_mapping_df['fb'])\n",
    "mapping = list(fb_mapping_df['string'])\n",
    "\n",
    "keys_fin = ['/'+w.replace('.','/') for w in keys]\n",
    "\n",
    "all_mids = list(set(train_df.Entity) | set(train_df.Tail))\n",
    "\n",
    "mapping_dict = dict(zip(keys_fin,mapping))\n",
    "\n",
    "keys_complete = list(set(keys_fin) & set(all_mids))\n",
    "\n",
    "fb15k_maps = [mapping_dict[x] for x in keys_complete]\n",
    "\n",
    "mapping_dict = dict(zip(keys_complete,fb15k_maps))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping_dict = dict(zip(fb15k_maps, keys_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dominican Republic'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_dict['/m/027rn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/m/027rn'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_mapping_dict['Dominican Republic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = fb_mapping_df.set_index('fb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_copy = test_df\n",
    "train_df_copy = train_df\n",
    "valid_df_copy = valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_mid(df):\n",
    "    \n",
    "    # Getting all possible entites\n",
    "    all_ents = df['Entity']\n",
    "    all_ents.append(df['Tail'])\n",
    "    \n",
    "    # Getting rid of duplicates\n",
    "    all_ents = list(set(all_ents))\n",
    "    \n",
    "    # Filtering out entites missing from the mapping file (not to many just throw them out)\n",
    "    matches = []\n",
    "    missing = []\n",
    "    \n",
    "    # Checking to see what's actually missing\n",
    "    for ent in all_ents:\n",
    "    \n",
    "        if 'm.' + ent[3:] in tmp.index:\n",
    "            matches.append(tmp.loc['m.' + ent[3:]])\n",
    "\n",
    "        else:\n",
    "            missing.append(ent)\n",
    "    \n",
    "    # Checking to see if all entities are accounted for (can comment out if it's annoying)\n",
    "    if(len(matches) + len(missing) == len(all_ents)):\n",
    "        \n",
    "        # Dropping missing entities\n",
    "        drop_index = []\n",
    "        \n",
    "        for i in range(len(df['Entity'])):\n",
    "\n",
    "            if df['Entity'][i] in missing:\n",
    "                drop_index.append(i)\n",
    "\n",
    "            elif df['Tail'][i] in missing:\n",
    "                drop_index.append(i)\n",
    "\n",
    "        cleaned_df = df.drop(drop_index, axis = 0)\n",
    "        \n",
    "        \n",
    "        for i in range(len(cleaned_df['Entity'])):\n",
    "            try:\n",
    "                tail = tmp.loc['m.' + cleaned_df['Tail'][i][3:]]\n",
    "                ent = tmp.loc['m.' + cleaned_df['Entity'][i][3:]]\n",
    "                cleaned_df['Entity'][i] = ent[0]\n",
    "                cleaned_df['Tail'][i] = tail[0]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return cleaned_df\n",
    "    else:\n",
    "        print('Something went wrong!!!')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned = translate_mid(test_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned = translate_mid(train_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cleaned = translate_mid(valid_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned.to_csv('convert_data/test_cleaned.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned.to_csv('convert_data/train_cleaned.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cleaned.to_csv('convert_data/valid_cleaned.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned = pd.read_csv('convert_data/test_cleaned.tsv', sep = '\\t')\n",
    "train_cleaned = pd.read_csv('convert_data/train_cleaned.tsv', sep = '\\t')\n",
    "valid_cleaned = pd.read_csv('convert_data/valid_cleaned.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned = train_cleaned.iloc[:473750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned = test_cleaned.iloc[:57869]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cleaned = valid_cleaned.iloc[:49026]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model using English Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_w2v = w2v.KeyedVectors.load_word2vec_format('english_wiki_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting df entities to wiki model format\n",
    "\n",
    "I found that all the items in FB15K are proper nouns i.e. names, titles, etc (the few that aren't can be represented as proper nouns). This makes the job pretty easy as we can add the POS constraint into the df so it easily works with the english wiki to generate some links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_wiki_vocab = list(wiki_w2v.vocab)\n",
    "english_wiki_vocab_prop  = [x for x in english_wiki_vocab if x[-6:] == '_PROPN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function converts entities into wiki form and back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity sub-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wiki_form(ent):\n",
    "    ent = ent.replace(' ', '::')\n",
    "    ent = ent + '_PROPN'\n",
    "    return ent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_name(ent):\n",
    "    ent = ent[:-6]\n",
    "    ent = ent.replace('::', ' ')\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2mid(ent):\n",
    "    try:\n",
    "        mid = reverse_mapping_dict[ent]\n",
    "    except:\n",
    "        mid = '--'\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting sub-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_wiki(_df):\n",
    "    \n",
    "    df = _df\n",
    "    # Run function on all entities in the DataFrame\n",
    "    df.Entity = df.Entity.apply(convert_to_wiki_form)\n",
    "    df.Tail = df.Tail.apply(convert_to_wiki_form)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_back(_df):\n",
    "    \n",
    "    df = _df\n",
    "    df.Entity = df.Entity.apply(convert_to_name)\n",
    "    df.Tail = df.Tail.apply(convert_to_name)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_back_to_names(_df):\n",
    "    df = _df\n",
    "    df.Entity = df.Entity.apply(name2mid)\n",
    "    df.Tail = df.Tail.apply(name2mid)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting to Wiki-Model Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test = format_wiki(test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train = format_wiki(train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_valid = format_wiki(valid_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dominican::Republic_PROPN</td>\n",
       "      <td>/location/country/form_of_government</td>\n",
       "      <td>Republic_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mighty::Morphin::Power::Rangers_PROPN</td>\n",
       "      <td>/tv/tv_program/regular_cast./tv/regular_tv_app...</td>\n",
       "      <td>Wendee::Lee_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle::Rodriguez_PROPN</td>\n",
       "      <td>/award/award_winner/awards_won./award/award_ho...</td>\n",
       "      <td>Naveen::Andrews_PROPN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Entity  \\\n",
       "0              Dominican::Republic_PROPN   \n",
       "1  Mighty::Morphin::Power::Rangers_PROPN   \n",
       "2              Michelle::Rodriguez_PROPN   \n",
       "\n",
       "                                            Relation                   Tail  \n",
       "0               /location/country/form_of_government         Republic_PROPN  \n",
       "1  /tv/tv_program/regular_cast./tv/regular_tv_app...      Wendee::Lee_PROPN  \n",
       "2  /award/award_winner/awards_won./award/award_ho...  Naveen::Andrews_PROPN  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Similarity Links \n",
    "Use the built in most_similar function which uses cos-similarity but in the Word2Vec Embeddings instead of the graph emdbeddings we used before. The goal is to enrich the graph with outside text sources such as wikipedia which Freebase is based off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wiki_sim_links(all_ents):\n",
    "    \n",
    "    ents = []; tails = []; conf = []\n",
    "    \n",
    "    for i in all_ents:\n",
    "        if (i in english_wiki_vocab_prop):\n",
    "            simlinks_for_i = [x for x in wiki_w2v.most_similar(i, topn  = 20)]\n",
    "            tails_to_add = [x[0] for x in simlinks_for_i]\n",
    "            conf_to_add = [x[1] for x in simlinks_for_i]\n",
    "            if len(simlinks_for_i) > 0:\n",
    "                tails.extend(tails_to_add)\n",
    "                ents.extend([i]*len(simlinks_for_i))\n",
    "                conf.extend(conf_to_add)\n",
    "        else:\n",
    "            continue\n",
    "    return pd.DataFrame(data = {'Entity': ents, 'Relation': ['/similar_to'] * len(ents), 'Tail': tails, 'Confidence': conf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_ents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-85362de012b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_sim_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_wiki_sim_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_ents' is not defined"
     ]
    }
   ],
   "source": [
    "test_sim_links = create_wiki_sim_links(all_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sim_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sim_links.to_csv('text_sim_links_fb15k.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Threshold for Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sim_links(file = 'text_sim_links_fb15k.tsv', threshold = 0.85):\n",
    "    links = pd.read_csv('text_sim_links_fb15k.tsv', sep = '\\t')\n",
    "    filtered_links = links[links['Confidence'] > threshold]\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_enriched = wiki_train.append(select_sim_links(threshold=0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_enriched = wiki_train_enriched.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474839"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_train_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_enriched = wiki_test.append(select_sim_links(threshold=0.8))\n",
    "wiki_test_enriched = wiki_test_enriched.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_enriched = wiki_test_enriched[wiki_test_enriched['Entity'] != '--']\n",
    "wiki_test_enriched = wiki_test_enriched[wiki_test_enriched['Tail'] != '--']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_train_enriched[wiki_train_enriched['Relation'] == '/similar_to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89484</th>\n",
       "      <td>Buffalo::Sabres_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Edmonton::Oilers_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27621</th>\n",
       "      <td>Salavat::Yulaev::Ufa_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Ak::Bars::Kazan_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94186</th>\n",
       "      <td>New::York::Jets_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Cincinnati::Bengals_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86103</th>\n",
       "      <td>June_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>August_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11062</th>\n",
       "      <td>Utah::Jazz_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Phoenix::Suns_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121825</th>\n",
       "      <td>Edmonton::Eskimos_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Saskatchewan::Roughriders_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106626</th>\n",
       "      <td>Buffalo::Bills_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Kansas::City::Chiefs_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76464</th>\n",
       "      <td>October_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>August_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87080</th>\n",
       "      <td>Dallas::Mavericks_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Houston::Rockets_PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69140</th>\n",
       "      <td>Chicago::Bears_PROPN</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>Philadelphia::Eagles_PROPN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Entity     Relation  \\\n",
       "89484        Buffalo::Sabres_PROPN  /similar_to   \n",
       "27621   Salavat::Yulaev::Ufa_PROPN  /similar_to   \n",
       "94186        New::York::Jets_PROPN  /similar_to   \n",
       "86103                   June_PROPN  /similar_to   \n",
       "11062             Utah::Jazz_PROPN  /similar_to   \n",
       "121825     Edmonton::Eskimos_PROPN  /similar_to   \n",
       "106626        Buffalo::Bills_PROPN  /similar_to   \n",
       "76464                October_PROPN  /similar_to   \n",
       "87080      Dallas::Mavericks_PROPN  /similar_to   \n",
       "69140         Chicago::Bears_PROPN  /similar_to   \n",
       "\n",
       "                                   Tail  \n",
       "89484            Edmonton::Oilers_PROPN  \n",
       "27621             Ak::Bars::Kazan_PROPN  \n",
       "94186         Cincinnati::Bengals_PROPN  \n",
       "86103                      August_PROPN  \n",
       "11062               Phoenix::Suns_PROPN  \n",
       "121825  Saskatchewan::Roughriders_PROPN  \n",
       "106626       Kansas::City::Chiefs_PROPN  \n",
       "76464                      August_PROPN  \n",
       "87080            Houston::Rockets_PROPN  \n",
       "69140        Philadelphia::Eagles_PROPN  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train_enriched[wiki_train_enriched['Relation'] == '/similar_to'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting from Wiki format to /m/id format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_to_mid(_df):\n",
    "    df = _df\n",
    "    df = format_back(df)\n",
    "    df = format_back_to_names(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_enriched = wiki_to_mid(wiki_train_enriched)\n",
    "wiki_test_enriched = wiki_to_mid(wiki_test_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/027rn</td>\n",
       "      <td>/location/country/form_of_government</td>\n",
       "      <td>/m/06cx9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/017dcd</td>\n",
       "      <td>/tv/tv_program/regular_cast./tv/regular_tv_app...</td>\n",
       "      <td>/m/06v8s0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/01sl1q</td>\n",
       "      <td>/award/award_winner/awards_won./award/award_ho...</td>\n",
       "      <td>/m/044mz_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/0cnk2q</td>\n",
       "      <td>/soccer/football_team/current_roster./sports/s...</td>\n",
       "      <td>/m/02nzb8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/m/02_j1w</td>\n",
       "      <td>/sports/sports_position/players./soccer/footba...</td>\n",
       "      <td>/m/01cwm1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entity                                           Relation       Tail\n",
       "0   /m/027rn               /location/country/form_of_government   /m/06cx9\n",
       "1  /m/017dcd  /tv/tv_program/regular_cast./tv/regular_tv_app...  /m/06v8s0\n",
       "2  /m/01sl1q  /award/award_winner/awards_won./award/award_ho...  /m/044mz_\n",
       "3  /m/0cnk2q  /soccer/football_team/current_roster./sports/s...  /m/02nzb8\n",
       "4  /m/02_j1w  /sports/sports_position/players./soccer/footba...  /m/01cwm1"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train_enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_enriched = wiki_train_enriched[wiki_train_enriched['Entity'] != '--']\n",
    "wiki_train_enriched = wiki_train_enriched[wiki_train_enriched['Tail'] != '--']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147824</th>\n",
       "      <td>/m/05g3b</td>\n",
       "      <td>/similar_to</td>\n",
       "      <td>/m/0289q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entity     Relation      Tail\n",
       "147824  /m/05g3b  /similar_to  /m/0289q"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train_enriched[wiki_train_enriched['Relation'] == '/similar_to'].sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Wiki-Enriched KB to Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_enriched.to_csv('wiki_enrich_data/wiki_train_enriched_09.txt', sep = '\\t', index = False)\n",
    "wiki_test_enriched.to_csv('wiki_enrich_data/wiki_test_enriched.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/027rn</td>\n",
       "      <td>/location/country/form_of_government</td>\n",
       "      <td>/m/06cx9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/017dcd</td>\n",
       "      <td>/tv/tv_program/regular_cast./tv/regular_tv_app...</td>\n",
       "      <td>/m/06v8s0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/01sl1q</td>\n",
       "      <td>/award/award_winner/awards_won./award/award_ho...</td>\n",
       "      <td>/m/044mz_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/0cnk2q</td>\n",
       "      <td>/soccer/football_team/current_roster./sports/s...</td>\n",
       "      <td>/m/02nzb8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/m/02_j1w</td>\n",
       "      <td>/sports/sports_position/players./soccer/footba...</td>\n",
       "      <td>/m/01cwm1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entity                                           Relation       Tail\n",
       "0   /m/027rn               /location/country/form_of_government   /m/06cx9\n",
       "1  /m/017dcd  /tv/tv_program/regular_cast./tv/regular_tv_app...  /m/06v8s0\n",
       "2  /m/01sl1q  /award/award_winner/awards_won./award/award_ho...  /m/044mz_\n",
       "3  /m/0cnk2q  /soccer/football_team/current_roster./sports/s...  /m/02nzb8\n",
       "4  /m/02_j1w  /sports/sports_position/players./soccer/footba...  /m/01cwm1"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train_enriched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining Rules \n",
    "Using AMIE again to mine rules from the wiki-enriched KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('java -XX:-UseGCOverheadLimit -Xmx4g -jar AMIE/amie_plus.jar -minhc 0.0 -mins 0 -minis 0 wiki_enrich_data/wiki_train_enriched.txt > rules/wiki_rules.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Remember to clean 'baseline_rules.txt' so it only includes the rule. Take out output from top and bottom so there are only rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('java -jar AMIE/ApplyAMIERules.jar rules/wiki_rules.txt wiki_enrich_data/wiki_train_enriched.txt FB15K/test.txt  FB15K/valid.txt evaluation/wiki_rules_eval.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@10: 0.7189822417091297\n"
     ]
    }
   ],
   "source": [
    "print('Hits@10: ' + str(eval_frame('evaluation/_rules_eval.txt', len(test_df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (Hits@10)\n",
    "\n",
    "### Graph Embeddings\n",
    "    - Threshold : 0.85  ----  Hits@10 : \n",
    "    - Threshold : 0.85  ----  Hits@10 :\n",
    "### Word2Vec Wiki Embeddings\n",
    "    - Threshold : 0.85  ----  Hits@10 : 0.71898 \n",
    "    - Threshold : 0.85  ----  Hits@10 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
